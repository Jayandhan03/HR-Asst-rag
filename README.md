---
title: HR Assistant Bot
emoji: ğŸ¤–
colorFrom: green
colorTo: blue
sdk: streamlit
sdk_version: "1.26.0"
app_file: app.py
pinned: false
# to run locally: python -m streamlit run app.py
---


# HR Assistant Bot

This Space allows employees to query HR policies using an AI-powered assistant.
ğŸ§  HR Policy Q&A Assistant (RAG-powered)

A Retrieval-Augmented Generation (RAG) system that intelligently answers employee HR policy questions using PDF documents. Built with LangChain and Qdrant, the assistant retrieves, reranks, and generates accurate answers from company policy files.

---

## ğŸ“‚ Folder Structure

```

â”œâ”€â”€ chunking/         # Semantic chunking logic
â”œâ”€â”€ data/             # HR policy PDFs
â”œâ”€â”€ embedding/        # Embedding models
â”œâ”€â”€ Final/            # Final runnable scripts
â”œâ”€â”€ ingest/           # Incremental ingestion pipeline
â”œâ”€â”€ interface/        # CLI / frontend setup (in progress)
â”œâ”€â”€ llm/              # LLM interaction & prompt templates
â”œâ”€â”€ Prompt/           # Prompt customization
â”œâ”€â”€ render/           # DOCX response renderer
â”œâ”€â”€ Reranker/         # BM25/MMR reranking
â”œâ”€â”€ retrieval/        # Retriever logic (Qdrant)
â”œâ”€â”€ Tracing/          # LangSmith/OpenTelemetry (observability)
â”œâ”€â”€ utils/            # Common utilities (logging, config, etc.)
â”œâ”€â”€ vectorstore/      # Qdrant index handling

````

---

## âœ… Features

- ğŸ“¥ **Incremental PDF ingestion**
- âœ‚ï¸ **Semantic chunking + embedding**
- ğŸ§  **Multi-index vector store (Flat, HNSW, IVF) using Qdrant**
- âš–ï¸ **BM25/MMR-based reranking for relevance**
- ğŸ’¬ **LLM-based direct answer generation**
- ğŸ§¾ **DOCX rendering of answers**
- ğŸ§  **Prompt templating support**
- ğŸ“¡ **LangSmith integration**
- ğŸ§  **Multi-turn memory (WIP)**
- ğŸŒ **Streamlit interface (planned)**
- ğŸ³ **Dockerized deployment (in progress)**

---

## ğŸš€ How It Works

1. Ingest HR PDFs and split them into semantically meaningful chunks
2. Embed the chunks using OpenAI or HuggingFace models
3. Store them in Qdrant with efficient vector indexing
4. Retrieve top-k documents using similarity search
5. Rerank results using BM25 or MMR
6. Use LLM with templated prompt to generate final response
7. Export response to DOCX

---

## ğŸ’» Usage

```bash
# Step 1: Install dependencies
pip install -r requirements.txt

# Step 2: Run CLI
python Final/Final_Pipeline.py --query "Is my spouse covered under the company health insurance?"
````

---

## ğŸ” Sample Output

**Q:** "How many casual leaves do employees get per year?"
**A:** "Yes, your legal spouse is eligible for coverage under our medical, dental, and vision plans. You will need to provide documentation to verify their eligibility."

---

## ğŸ§° Tech Stack

* **LangChain**
* **Qdrant**
* **OpenAI / Ollama / HuggingFace**
* **BM25 / MMR**
* **LangSmith**
* **Python**, **Docker**

---

## ğŸ› ï¸ Planned Improvements

* âœ… Streamlit / Gradio UI
* âœ… Redis/SQLite-based chat memory
* âœ… Docker + cloud deployment DOCKER COMMAND - {"http://localhost:8501/",docker run --env-file .env -p 8501:8501 hr-assistant-bot:latest}
* âœ… Slack/MS Teams integration

---

## ğŸ‘¤ Author

**Jayandhan S** â€” Passionate about building agentic GenAI systems and real-world AI assistants.

---

## ğŸ“œ License

MIT License

```


